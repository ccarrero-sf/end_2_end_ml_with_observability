{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "hvisuxf4kbod5s63oze2",
   "authorId": "5744486210470",
   "authorName": "CCARRERO",
   "authorEmail": "carlos.carrero@snowflake.com",
   "sessionId": "afd03ce6-70df-44aa-ae26-ca6064b7ad9d",
   "lastEditTime": 1744114681971
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\nfrom snowflake.snowpark.types import DecimalType, FloatType, IntegerType, DoubleType, LongType\nnumeric_types = (DecimalType, FloatType, IntegerType, DoubleType, LongType)\n\nimport logging\nlogger = logging.getLogger(\"e2e-churn-log\")\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2231df97-c719-4575-8ad0-0a35c4cac697",
   "metadata": {
    "name": "cell60",
    "collapsed": false
   },
   "source": "## Setting Up the Environment"
  },
  {
   "cell_type": "code",
   "id": "c678832d-9d0c-4a84-839f-b2669c82dfe9",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# Change the schema name to work in a different env\n\nSCHEMA = 'TEST1'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "source": "\nsession.sql(f'CREATE OR REPLACE SCHEMA {SCHEMA} ').collect()\nlogger.info(f'Created new schema {SCHEMA}.')\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "062a1a7d-3731-4b2b-aa02-0b6934de8fdc",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": "session.sql (f'USE SCHEMA {SCHEMA} ').collect()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2282090b-8b73-40be-b7e8-8fa97b9b0770",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": "session.sql ('CREATE OR REPLACE STAGE ML_STAGE').collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f829f0a8-93c3-4e6f-9c6b-2d6ae75482cb",
   "metadata": {
    "language": "python",
    "name": "cell63"
   },
   "outputs": [],
   "source": "# Columns we are going to be using for training\n\noe_input_cols = ['GENDER', 'LOCATION', 'CUSTOMER_SEGMENT']\n\nss_input_numerical_cols = [\n    \"AGE\",\n    \"SENTIMENT_MIN_2\", \"SENTIMENT_MIN_3\", \"SENTIMENT_MIN_4\", \"SENTIMENT_AVG_2\",\n    \"SENTIMENT_AVG_3\", \"SENTIMENT_AVG_4\",\n    \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n    \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n]\ncolumns = oe_input_cols + ss_input_numerical_cols",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "sql",
    "name": "create_tables",
    "codeCollapsed": false
   },
   "source": "CREATE or replace TABLE SALES (\n    TRANSACTION_ID VARCHAR,\n    CUSTOMER_ID VARCHAR,\n    TRANSACTION_DATE DATE,\n    DISCOUNT_APPLIED BOOLEAN,\n    NUM_ITEMS NUMBER,\n    PAYMENT_METHOD VARCHAR, \n    TOTAL_AMOUNT FLOAT\n);\n\nCREATE or replace TABLE CUSTOMERS (\n    CUSTOMER_ID VARCHAR,\n    AGE BIGINT,\n    CUSTOMER_SEGMENT VARCHAR,\n    GENDER VARCHAR,\n    LOCATION VARCHAR,\n    SIGNUP_DATE DATE\n);\n\nCREATE or replace TABLE FEEDBACK_RAW (\n    CHAT_DATE DATE,\n    COMMENT VARCHAR,\n    CUSTOMER_ID VARCHAR,\n    FEEDBACK_ID VARCHAR,\n    INTERNAL_ID BIGINT\n);\n\nCREATE or replace STREAM FEEDBACK_RAW_STREAM \n    ON TABLE FEEDBACK_RAW\n    APPEND_ONLY = TRUE;\n\nCREATE or replace TABLE FEEDBACK_SENTIMENT (\n    FEEDBACK_ID VARCHAR,\n    CHAT_DATE DATE,\n    CUSTOMER_ID VARCHAR,\n    INTERNAL_ID BIGINT,\n    COMMENT VARCHAR,\n    SENTIMENT FLOAT\n);\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ee44c474-d7b3-4697-b10b-5f285c73c4c6",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "-- Us this table to track what monthly files we have processed\n\nCREATE OR REPLACE TABLE FILES_INGESTED (\n    YEAR INT,\n    MONTH INT,\n    FILE_TYPE VARCHAR,\n    FILE_NAME VARCHAR,\n    STAGE_NAME VARCHAR,\n    INGESTED BOOLEAN\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a7e41a8-91c4-4872-95fd-7e35a175e923",
   "metadata": {
    "language": "python",
    "name": "def_get_year_month_files"
   },
   "outputs": [],
   "source": "# Read from the staging area the files that we have\n\nimport re\nfrom snowflake.snowpark import Session\nfrom typing import List, Tuple\n\ndef get_year_month_files(session: Session, stage_name: str, file_prefix: str) -> List[Tuple[int, int, str]]:\n    # List files in the stage\n    list_files_query = f\"LIST @{stage_name}\"\n    files = session.sql(list_files_query).collect()\n\n    # Extract file names\n    file_names = [file[\"name\"].split(\"/\")[-1] for file in files]\n\n    # Regular expression to extract year and month from filenames\n    file_pattern = re.compile(rf\"{re.escape(file_prefix)}_(\\d+)_(\\d+)\\.csv\")\n\n    # List to store (year, month, filename) tuples\n    results = []\n\n    for file_name in file_names:\n        match = file_pattern.match(file_name)\n        if match:\n            year, month = int(match.group(1)), int(match.group(2))\n            results.append((year, month, file_name, stage_name))\n\n    # Sort the list by (year, month)\n    return sorted(results)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae74ff47-1572-4143-b82e-b1a1d411ab10",
   "metadata": {
    "language": "python",
    "name": "exec_insert_files"
   },
   "outputs": [],
   "source": "# Function to insert into the FILES_INGESTED table the files that we will process\n\nstage_name = \"DATASET.CSV\"\n\ndb = session.get_current_database()\nsc = session.get_current_schema()\nprint (f'database: {db}, schema: {sc}')\n\n\ndef insert_files (table,db, sc, files):\n    for file in files:\n        year = file[0]\n        month = file[1]\n        file_name = file[2]\n        stage_name = file[3]\n        sql_cmd = f\"\"\"\n            insert into  {db}.{sc}.FILES_INGESTED\n             (YEAR, month, file_type, file_name, stage_name, ingested)\n             values \n                ('{year}', '{month}', '{table}', '{file_name}','{stage_name}', False)\n        \"\"\"\n        session.sql(sql_cmd).collect()  \n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9cc40f2-0d45-4f7b-83f4-9e415b1777cb",
   "metadata": {
    "language": "python",
    "name": "cell48"
   },
   "outputs": [],
   "source": "sales_month_files = get_year_month_files(session, stage_name, 'sales')\ninsert_files ('sales', db, sc, sales_month_files)\n\nfeedback_sentiment_month_files = get_year_month_files(session, stage_name, 'feedback_raw')\ninsert_files ('feedback_raw',db, sc, feedback_sentiment_month_files)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9bb44d51-9aac-44d3-a9b4-ced1816240f8",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "select * from FILES_INGESTED;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cbf0e857-60d0-419e-bd69-f5229733a454",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "# For a simple demo, the customers are fixed, we do not ingest them in a monthly basis\n\nsql_cmd = f\"\"\"\n    COPY INTO {db}.{sc}.customers  \n            FROM @{stage_name}/customers.csv  \n            FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"')  \n            ON_ERROR = 'CONTINUE'; \n\"\"\"\n\nsession.sql(sql_cmd).collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "47549ff6-d4d4-4429-af7f-8329e2aadc99",
   "metadata": {
    "language": "python",
    "name": "cell64"
   },
   "outputs": [],
   "source": "# Function to copy from the staging area the CSV file into sales and feedback_raw tables\n# Copy the next file not yet ingested into the tables\n\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark import types as T\n\ndef load_into_table(session, table_name, file_name):\n\n    sql_cmd = f\"\"\" \n        COPY INTO {table_name}\n            FROM {file_name}  \n            FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"')  \n            ON_ERROR = 'ABORT_STATEMENT';      \n\n    \"\"\"\n    session.sql(sql_cmd).collect()\n\ndef copy_next_file( session: Session, db: str, sc: str):\n    \n    df = session.table(f'{db}.{sc}.FILES_INGESTED')\n    \n    df_sales = df\\\n        .filter((F.col(\"file_type\") == 'sales') & (F.col(\"ingested\") == False)) \\\n        .select(F.col(\"year\"), F.col(\"month\"), F.col(\"file_name\"), F.col(\"stage_name\")) \\\n        .order_by(F.col(\"year\").asc(), F.col(\"month\").asc()) \\\n        .limit(1)\n    \n    df_pd = df_sales.to_pandas()\n\n    if df_pd.empty:\n        print(\"No unprocessed sales files found.\")\n        return False\n    \n    year = int(df_pd.YEAR[0])\n    month = int(df_pd.MONTH[0])\n    file_name = df_pd.FILE_NAME[0]\n    stage_name = df_pd.STAGE_NAME[0]\n    file_name_path = f'@{stage_name}/{file_name}'\n    full_table_name = f'{db}.{sc}.sales'\n    load_into_table(session, full_table_name, file_name_path)\n\n    # Update FILES_INGESTED to mark this file as ingested\n    update_cmd = f\"\"\"\n        UPDATE {db}.{sc}.FILES_INGESTED\n        SET ingested = TRUE\n        WHERE file_name = '{file_name}' AND file_type = 'sales'\n    \"\"\"\n    session.sql(update_cmd).collect()\n    print(f\"Updated FILES_INGESTED for file: {file_name}\")\n\n    \n    print (year, month)\n    \n    df_feedback = df\\\n        .filter((F.col(\"file_type\") == 'feedback_raw') & \\\n                (F.col(\"ingested\") == False) &\\\n                (F.col(\"YEAR\") == F.lit(year)) &\\\n                (F.col(\"MONTH\") == F.lit(month))) \\\n        .select(F.col(\"year\"), F.col(\"month\"), F.col(\"file_name\"), F.col(\"stage_name\")) \\\n        .order_by(F.col(\"year\").asc(), F.col(\"month\").asc()) \\\n        .limit(1)\n    \n    if (df_feedback.count() > 0):\n        df_pd = df_feedback.to_pandas()\n        file_name = df_pd.FILE_NAME[0]\n        stage_name = df_pd.STAGE_NAME[0]\n        file_name_path = f'@{stage_name}/{file_name}'\n        full_table_name = f'{db}.{sc}.feedback_raw'   \n        load_into_table(session, full_table_name, file_name_path)\n\n       # Update FILES_INGESTED to mark this file as ingested\n        update_cmd = f\"\"\"\n            UPDATE {db}.{sc}.FILES_INGESTED\n            SET ingested = TRUE\n            WHERE file_name = '{file_name}' AND file_type = 'feedback_raw'\n        \"\"\"\n        session.sql(update_cmd).collect()\n        print(f\"Updated FILES_INGESTED for file: {file_name}\")\n\n\n    return True\n\n#session.sproc.register(\n#    func=copy_next_file,\n#    name=\"copy_next_file_sproc\",\n#    replace=True,\n#    is_permanent=True,\n#    stage_location=\"@ML_STAGE\",\n#    packages=['snowflake-snowpark-python'],\n#    return_type=T.BooleanType()\n#)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f32ba42-191b-43fb-91b0-18d972c883af",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "#### Cortex AI to process Unstructured data\n\nTake advantage of the power of LLMs to provide a number (sentiment) for the feedback received in text from the customer. This will be used as one feature to train our ML models (and later predict)"
  },
  {
   "cell_type": "code",
   "id": "022811ea-9355-47d5-95a7-2466f8c780c3",
   "metadata": {
    "language": "python",
    "name": "cell66",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Leverage sentiment function\n\nfrom snowflake.cortex import sentiment\n\ndef process_sentiment():\n\n    # Read from the Snowflake stream instead of the raw table\n    feedback_stream_df = session.table(\"feedback_raw_stream\")  # The stream on feedback_raw\n\n    cols = ['FEEDBACK_ID', 'CHAT_DATE', 'CUSTOMER_ID', 'INTERNAL_ID', 'COMMENT']\n    \n    # Apply sentiment analysis using Cortex AI:\n    \n    feedback_sentiment_df = feedback_stream_df.select(cols).with_columns(\n        [\"sentiment\"], [sentiment(F.col(\"comment\"))]\n    )\n   \n    # Write processed data into the target table\n    feedback_sentiment_df.write.mode(\"append\").save_as_table(\"feedback_sentiment\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11831e4d-d82b-4423-9c17-79e6e983c798",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "# Tables that will container the features. Second one contains the labels\n\nstage_name = \"DATASET.CSV\"\nchurn_window = 30 ## This is the value we define as churn\ntable_features = 'churn_baseline'\ntable_features_labeled = 'churn_baseline_labeled'\n\nsession.sql(f'drop table if exists {table_features} ').collect() #fresh start for this demo\nsession.sql(f'drop table if exists {table_features_labeled} ').collect() #fresh start for this demo\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdc70499-ff02-4543-a687-da9e9f395e9e",
   "metadata": {
    "language": "python",
    "name": "load_first_data",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Loading first 4 months that will be used for training the first model\n# Load the file for sales and feedback\n# Calculate the features for the latest sales timestamp for each file\n# Label the CHURN based on feature transactions\n\nfrom snowflake.snowpark import functions as F\nfrom datetime import datetime\n\ndb = session.get_current_database()\nsc = session.get_current_schema()\nprint (f'database: {db}, schema: {sc}')\n\n# Ingest and process the frist 4 files:\nfor i in range (0,4):\n    t1 = datetime.now()\n    \n    # STEP 1: COPY NEXT SALES AND FEEDBACK MONTH\n    print (\"#############################\")\n    print (\"Ingest the next sales and feedback files\")\n    copy_next_file (session, db, sc)\n    \n    # STEP 2: PROCESS THE FEEDBACK RECEIVED\n    print (\"Processing Sentiment\")\n    process_sentiment()\n    \n    t2 = datetime.now()\n    print (t2 - t1)\n    \n    # STEP 3: CALCULATE FEATURES FOR LATEST TIMESTAMP\n    sales_df = session.table(\"sales\") \n    \n    #calculate features for the latest transaction timestamp\n    latest_transaction = sales_df.select(F.max(F.col(\"transaction_date\"))).collect()[0][0]\n    \n    print (f'Calculating features for timestamp : {latest_transaction} ')\n    session.call('UTILS.uc01_feature_engineering_sproc', db, sc, latest_transaction, table_features)\n    \n    t3 = datetime.now()\n    print (t3 - t2)\n       \n    # STEP 4: SET RIGHT CHURN LABEL BASED ON NEW SALES DATA\n    \n    print (f'Adding labels for window: {churn_window} ')\n    session.call('UTILS.uc_01_label_churn_sproc', db, sc, table_features, table_features_labeled, 30 )\n\n    t4 = datetime.now()\n    print (t4 - t3)\n    \n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb1888ef-c213-455a-bcb6-5f5dc58435f6",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# Look at the balance. We will use the second one for training and the thrid for validation. \n# Note that labels for the last one are incorrect as wew still need the next month to have real values\n\nsql_cmd = f\"\"\"\n            SELECT \n                TIMESTAMP,\n                SUM(CASE WHEN churned = 0 THEN 1 ELSE 0 END) AS not_churned,\n                SUM(CASE WHEN churned = 1 THEN 1 ELSE 0 END) AS churned\n            FROM {table_features_labeled}\n            GROUP BY TIMESTAMP\n            ORDER BY TIMESTAMP;\n            \"\"\"\nsession.sql(sql_cmd).collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e2b214c-038f-4587-8736-66195f4d7e4d",
   "metadata": {
    "language": "sql",
    "name": "cell39"
   },
   "outputs": [],
   "source": "-- Let's take a look to one record to see how itlooks like\n\nselect TIMESTAMP, CUSTOMER_ID, LAST_PURCHASE_DATE, DAYS_SINCE_LAST_PURCHASE,  NEXT_TRANSACTION_DATE\n, NEXT_TRANSACTION_DATE - LAST_PURCHASE_DATE ,  CHURNED from churn_baseline_labeled  where customer_id = 'CUST-11'\nORDER BY TIMESTAMP;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "57c9126e-d29c-4a32-b06e-f1873abbd4d3",
   "metadata": {
    "name": "cell61",
    "collapsed": false
   },
   "source": "## Feature Store and Model Registry"
  },
  {
   "cell_type": "code",
   "id": "80a6154d-9997-4438-94d2-a4899a7c4b49",
   "metadata": {
    "language": "python",
    "name": "def_fs_mr"
   },
   "outputs": [],
   "source": "# Create the Feature Store and Model Registry. Will use the sufix name of the SCHEMA being used by this Notebook\n\nfrom snowflake.ml.feature_store import (\n    FeatureStore,\n    FeatureView,\n    Entity,\n    CreationMode)\n\n# Snowflake Model Registry\nfrom snowflake.ml.registry import Registry\n\ndb = session.get_current_database()\nsc = session.get_current_schema()\nprint (sc)\n\nmr_schema = f'{sc}_MODEL_REGISTRY'\nmr_schema = mr_schema.replace('\"', '')\nfs_schema = f'{sc}_FEATURE_STORE'\nfs_schema = fs_schema.replace('\"', '')\n\nwarehouse = 'COMPUTE_WH'  #modify as needed.This one is standard in quickstarts\n\nprint (mr_schema)\n\n#cleanup - When running this notebook, we are starting from scratch for the demo\n\nsession.sql(f'drop schema if exists {mr_schema}').collect()\nsession.sql(f'drop schema if exists {fs_schema}').collect()\n\n\n# Create the Model Registry\ntry:\n    cs = session.get_current_schema()\n    session.sql(f''' create schema {mr_schema} ''').collect()\n    mr = Registry(session=session, database_name= db, schema_name=mr_schema)\n    session.sql(f''' use schema {cs}''').collect()\nexcept:\n    print(f\"Model Registry ({mr_schema}) already exists\")   \n    mr = Registry(session=session, database_name= db, schema_name=mr_schema)\nelse:\n    print(f\"Model Registry ({mr_schema}) created\")\n\n\n# Create the Feature Store\ntry:\n    fs = FeatureStore(session=session, database=db, name=fs_schema, \n                          default_warehouse=warehouse, \n                          creation_mode=CreationMode.FAIL_IF_NOT_EXIST)\n    print(f\"Feature Store ({fs_schema}) already exists\") \nexcept:\n    fs = FeatureStore(session=session, database=db, name=fs_schema, \n                          default_warehouse=warehouse, \n                          creation_mode=CreationMode.CREATE_IF_NOT_EXIST)\n    print(f\"Feature Store ({fs_schema}) created\")   \n\n\nsession.use_schema(sc)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78f2ff05-245a-4a89-ba17-41319d8e57f3",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "### Feature Store: Entity -> Feature View -> Dataset"
  },
  {
   "cell_type": "code",
   "id": "9564de90-9ac3-4547-a196-2dac48caf611",
   "metadata": {
    "language": "python",
    "name": "def_entities"
   },
   "outputs": [],
   "source": "# Define Entity\n\nimport json\n\nif \"CUSTOMER_ENT\" not in json.loads(fs.list_entities().select(F.to_json(F.array_agg(\"NAME\", True))).collect()[0][0]):\n    customer_entity = Entity(\n        name=\"CUSTOMER_ENT\", \n        join_keys=[\"CUSTOMER_ID\"],\n        desc=\"Primary Key for CUSTOMER\")\n    fs.register_entity(customer_entity)\nelse:\n    customer_entity = fs.get_entity(\"CUSTOMER_ENT\")\n\nfs.list_entities().show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b45116c0-2eef-4e8e-8a47-c09462e73b3f",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "Define the Feature View based on the table where features are being calculated."
  },
  {
   "cell_type": "code",
   "id": "6864855a-9c27-4bb9-8a07-7e617954f32f",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "sc = session.get_current_schema()\n\nchurn_df = session.table(f'{sc}.{table_features_labeled}')\n\npreprocess_features_desc = {  \n}\n\nppd_fv_name    = \"FV_UC01_PREPROCESS\"\nppd_fv_version = \"V_1\"\n\ntry:\n   # If FeatureView already exists just return the reference to it\n   fv_uc01_preprocess = fs.get_feature_view(name=ppd_fv_name,version=ppd_fv_version)\nexcept:\n   # Create the FeatureView instance\n   fv_uc01_preprocess_instance = FeatureView(\n      name=ppd_fv_name, \n      entities=[customer_entity], \n      feature_df=churn_df,      # <- We can use the snowpark dataframe as-is from our Python\n      timestamp_col=\"TIMESTAMP\",\n      refresh_freq=None,  # The refresh will be external to feature view (maintained in the pipeline)\n      desc=\"Features to support Churn Detection\").attach_feature_desc(preprocess_features_desc)\n\n   # Register the FeatureView instance.  Creates  object in Snowflake\n   fv_uc01_preprocess = fs.register_feature_view(\n      feature_view=fv_uc01_preprocess_instance, \n      version=ppd_fv_version, \n      block=True\n   )\n   print(f\"Feature View : {ppd_fv_name}_{ppd_fv_version} created\")   \nelse:\n   print(f\"Feature View : {ppd_fv_name}_{ppd_fv_version} already created\")\nfinally:\n   fs.list_feature_views().show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aaa036bd-b53e-4998-a3d5-50802c3ffeff",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "# We can take a look to how the features looks like:\n\nfv_uc01_preprocess.feature_df.filter(F.col(\"CUSTOMER_ID\") == \"CUST-1\").show(5)\nfv_uc01_preprocess.feature_df.show(5)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64d79293-759b-4701-9c85-05e71d4ea9c2",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "# Look at the balance. We will use the second one for training and the thrid for validation. \n# Note that labels for the last one are incorrect as wew still need the next month to have real values\n\nsql_cmd = f\"\"\"\n            SELECT \n                TIMESTAMP,\n                SUM(CASE WHEN churned = 0 THEN 1 ELSE 0 END) AS not_churned,\n                SUM(CASE WHEN churned = 1 THEN 1 ELSE 0 END) AS churned\n            FROM {table_features_labeled}\n            GROUP BY TIMESTAMP\n            ORDER BY TIMESTAMP;\n            \"\"\"\nsession.sql(sql_cmd).collect()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d4b3fb9-d5b3-48ea-a549-2a6b8a16bbb2",
   "metadata": {
    "language": "python",
    "name": "timestamp_training_val"
   },
   "outputs": [],
   "source": "# Take the timestamps we are going to be used for training and validation\n# Note both of them should have real labels so we can measure performance\n\ntimestamps = session.table(table_features_labeled).select(\"TIMESTAMP\").distinct().sort(\"TIMESTAMP\").collect()\n\ntimestamp_training = timestamps[1][\"TIMESTAMP\"]\ntimestamp_testing = timestamps[2][\"TIMESTAMP\"]\n\nprint (f'Timestamp used for training: {timestamp_training} ')\nprint (f'Timestamp used for validation: {timestamp_testing} ')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a9b5524-5ed0-498d-8ed6-6234675e0c66",
   "metadata": {
    "language": "python",
    "name": "def_fs_generate_dataset"
   },
   "outputs": [],
   "source": "# Common function to generate and return a dataset\n\ndef fs_generate_dataset (fs, fv, name, timestamp):\n        \n    spine_sdf =  fv.feature_df.filter(F.col(\"TIMESTAMP\") == F.lit(timestamp)) \\\n                                .group_by('CUSTOMER_ID').agg(F.max('TIMESTAMP').as_('TIMESTAMP'))\n    \n    \n    dataset = fs.generate_dataset( name = name, version='v1',\n                                            spine_df = spine_sdf, features = [fv_uc01_preprocess], \n                                            spine_timestamp_col = 'TIMESTAMP'\n                                            )                                     \n    # Create a snowpark dataframe reference from the Dataset\n    dataset_sdf = dataset.read.to_snowpark_dataframe()\n    \n    decimal_columns = [ field.name for field in dataset_sdf.schema.fields\n            if isinstance(field.datatype, numeric_types)]\n    \n    for column_name in decimal_columns:\n        dataset_sdf = dataset_sdf.with_column(\n            column_name,\n            F.col(column_name).cast(\"float\")  # or \"float\" or DoubleType()\n        )\n    \n    return dataset_sdf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf154064-e9ee-4a12-8133-e2fa249dbd14",
   "metadata": {
    "language": "python",
    "name": "cell67"
   },
   "outputs": [],
   "source": "training_dataset_sdf = fs_generate_dataset(fs, fv_uc01_preprocess, 'UC01_TRAINING_INITIAL', timestamp_training)\ntesting_dataset_sdf = fs_generate_dataset(fs, fv_uc01_preprocess, 'UC01_VALIDATION', timestamp_testing)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "917c4ca6-195d-4fc9-bb91-69410b8d10d0",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "## Training\n\nWe are going to explore first different ways to train models within Snowflake and the Container Runtime, which also offers new capabilities for:\n- optimized data ingestion: https://docs.snowflake.com/en/developer-guide/snowflake-ml/container-runtime-ml#optimized-data-loading\n- Distributed Model Training: https://docs.snowflake.com/en/developer-guide/snowflake-ml/container-runtime-ml#optimized-training"
  },
  {
   "cell_type": "code",
   "id": "5d3b9ab9-3c62-48e1-bed7-3d8fc9c18412",
   "metadata": {
    "language": "python",
    "name": "snowflake_ml_training"
   },
   "outputs": [],
   "source": "## Classic Snowflake ML using ml.modeling libraries\n\nfrom snowflake.ml.modeling.preprocessing import StandardScaler as sml_StandardScaler\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder as sml_OrdinalEncoder\nfrom snowflake.ml.modeling.pipeline import Pipeline as sml_Pipeline\nfrom snowflake.ml.modeling import metrics as snowml_metrics\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\n\nfrom snowflake.snowpark.types import DecimalType, FloatType, IntegerType, DoubleType, LongType\nnumeric_types = (DecimalType, FloatType, IntegerType, DoubleType, LongType)\n\ndef uc01_train(feature_df):\n\n    decimal_columns = [ field.name for field in feature_df.schema.fields\n            if isinstance(field.datatype, numeric_types)]\n\n    for column_name in decimal_columns:\n        feature_df = feature_df.with_column(\n            column_name,\n            F.col(column_name).cast(\"float\")  # or \"float\" or DoubleType()\n            )\n    \n    train_df, testing_df = feature_df.random_split(weights=[0.8, 0.2], seed=111)\n    \n    oe_input_cols = ['GENDER', 'LOCATION', 'CUSTOMER_SEGMENT']\n    oe_output_cols = ['GENDER_OE', 'LOCATION_OE', 'CUSTOMER_SEGMENT_OE']\n\n    ss_input_numerical_cols = [\n        \"AGE\",\n        \"SENTIMENT_MIN_2\", \"SENTIMENT_MIN_3\", \"SENTIMENT_MIN_4\", \"SENTIMENT_AVG_2\",\n        \"SENTIMENT_AVG_3\", \"SENTIMENT_AVG_4\",\n        \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n        \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n    ]\n    \n    ss_output_numerical_cols = [col + \"_SS\" for col in ss_input_numerical_cols]\n    \n    label = ['CHURNED']\n    output_label = ['CHURNED_PRED']\n\n    input_columns = oe_input_cols + ss_input_numerical_cols + label\n    output_columnss = oe_output_cols + ss_output_numerical_cols\n    \n    pipeline_purchases = sml_Pipeline(\n        steps=[ (\"OE\",\n                    sml_OrdinalEncoder(\n                        input_cols=oe_input_cols,\n                        output_cols=oe_output_cols)),\n                (\"SS\",\n                    sml_StandardScaler(\n                        input_cols=ss_input_numerical_cols,\n                        output_cols=ss_output_numerical_cols)),\n                (\"XGB\",\n                    XGBClassifier(\n                        input_cols=output_columnss,\n                        label_cols=label,\n                        output_cols=output_label))\n                ])\n \n    pipeline_purchases.fit(train_df.select(input_columns))\n\n    predictions = pipeline_purchases.predict(train_df)\n\n    \n    train_f1_score = snowml_metrics.f1_score (df=predictions, \n                                                    y_true_col_names=['CHURNED'],\n                                                    y_pred_col_names=['CHURNED_PRED'])   \n\n    predictions = pipeline_purchases.predict(testing_df)\n    \n    test_f1_score = snowml_metrics.f1_score (df=predictions, \n                                                    y_true_col_names=['CHURNED'],\n                                                    y_pred_col_names=['CHURNED_PRED'])   \n\n    \n    return {'MODEL': pipeline_purchases,\n            'train_f1_score': train_f1_score ,\n            'test_f1_score': test_f1_score\n           }\n     ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7946f70-049c-4f05-a9f7-23245a4b62f5",
   "metadata": {
    "language": "python",
    "name": "cell26",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "trained_model = uc01_train(training_dataset_sdf)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50a2310e-b199-40d2-885a-ee71e301cea7",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": "trained_model\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ffc46109-72ba-4cfd-8c25-2015c1803b5a",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": "# As we have included the pre-preprocesing + Model within the pipeline, we can call predict\n# using a different dataset (this is for the next month that the model has not seen yet)\nmodel= trained_model['MODEL']\n\npredictions = model.predict(testing_dataset_sdf)\n\ntesting_f1_score = snowml_metrics.f1_score (df=predictions, \n                                                y_true_col_names=['CHURNED'],\n                                                y_pred_col_names=['CHURNED_PRED'])   \n\nprint (f'testing_f1_score = {testing_f1_score}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b0b7442-bc46-42b5-b599-52bf6174322b",
   "metadata": {
    "language": "python",
    "name": "def_oss_train"
   },
   "outputs": [],
   "source": "# As we are performing the training using CPUs or GPUs (depending\n# where we are running this notebook), we are going to use just OSS packages\n# given that the datasets are not big and will fit well in our compute resourdes\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score\nfrom xgboost import XGBClassifier\n\ndef uc01_oss_train(feature_df):\n\n    decimal_columns = [ field.name for field in feature_df.schema.fields\n            if isinstance(field.datatype, numeric_types)]\n\n    for column_name in decimal_columns:\n        feature_df = feature_df.with_column(\n            column_name,\n            F.col(column_name).cast(\"float\")  # or \"float\" or DoubleType()\n            )\n\n    feature_df = feature_df.to_pandas()\n    \n    # Split data into train and test sets\n    train_df, testing_df = train_test_split(feature_df, test_size=0.2, random_state=111)\n    \n    oe_input_cols = ['GENDER', 'LOCATION', 'CUSTOMER_SEGMENT']\n\n    ss_input_numerical_cols = [\n        \"AGE\",\n        \"SENTIMENT_MIN_2\", \"SENTIMENT_MIN_3\", \"SENTIMENT_MIN_4\", \"SENTIMENT_AVG_2\",\n        \"SENTIMENT_AVG_3\", \"SENTIMENT_AVG_4\",\n        \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n        \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n    ]\n      \n    label = ['CHURNED']\n \n    # Create the preprocessing and model pipeline\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"ordinal\", OrdinalEncoder(), oe_input_cols),\n            (\"scaler\", StandardScaler(), ss_input_numerical_cols)\n        ]\n    )\n\n    # Create the model pipeline\n    pipeline_purchases = Pipeline(\n        steps=[ \n            (\"preprocessor\", preprocessor),\n            (\"model\", XGBClassifier())\n        ]\n    )\n    \n    # Split the training data into X (features) and y (target)\n    X_train = train_df[oe_input_cols + ss_input_numerical_cols]\n    y_train = train_df[label]\n\n    # Train the pipeline\n    pipeline_purchases.fit(X_train, y_train)\n    \n    # Predictions on the training set\n    train_predictions = pipeline_purchases.predict(X_train)\n    \n    # Compute training F1 score\n    train_f1_score = f1_score(y_train, train_predictions)\n\n    # Split the test data into X (features) and y (target)\n    X_test = testing_df[oe_input_cols + ss_input_numerical_cols]\n    y_test = testing_df[label]\n    \n    # Predictions on the testing set\n    test_predictions = pipeline_purchases.predict(X_test)\n    \n    # Compute test F1 score\n    test_f1_score = f1_score(y_test, test_predictions)\n    \n    return {\n        'MODEL': pipeline_purchases,\n        'train_f1_score': train_f1_score,\n        'test_f1_score': test_f1_score\n    }\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ecad0d8d-18ae-4d54-b9db-8e1109a08c25",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "trained_oss_model = uc01_oss_train(training_dataset_sdf)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d1c7edf-c48c-438f-a57f-22c77a2cfa1a",
   "metadata": {
    "language": "python",
    "name": "cell47"
   },
   "outputs": [],
   "source": "trained_oss_model",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81986fce-d0b1-4e9d-8477-06d94e72f018",
   "metadata": {
    "language": "python",
    "name": "cell56"
   },
   "outputs": [],
   "source": "# As we have included the pre-preprocesing + Model within the pipeline, we can call predict\n# using a different dataset (this is for the next month that the model has not seen yet)\nmodel= trained_oss_model['MODEL']\n\ndf = testing_dataset_sdf.to_pandas()\n\nval_predictions = model.predict(df)\n\nlabel = ['CHURNED']\n\ny_val = df[label]\n \n# Compute validation F1 score\ntesting_f1_score = f1_score(y_val, val_predictions)\n\nprint (f'testing_f1_score = {testing_f1_score}')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a9b1c8c7-5080-4753-8697-f7a5f069e33e",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "### Parallel Hyperparameter Optimization (HPO) on Container Runtime for ML\n\nLet's explore how we can run HPO in the Container Runtime\nhttps://docs.snowflake.com/en/developer-guide/snowflake-ml/container-hpo\n"
  },
  {
   "cell_type": "code",
   "id": "6323d51e-6abc-4b11-803d-29b16eaff53f",
   "metadata": {
    "language": "python",
    "name": "hpo_training"
   },
   "outputs": [],
   "source": "# Here we are going to separate the pipeline pre-processing from model training:\n\nfrom snowflake.ml.modeling.preprocessing import StandardScaler as sml_StandardScaler\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder as sml_OrdinalEncoder\nfrom snowflake.ml.modeling.pipeline import Pipeline as sml_Pipeline\nfrom snowflake.ml.data.data_connector import DataConnector\nfrom snowflake.ml.modeling import tune\nfrom snowflake.ml.modeling.tune import get_tuner_context\nimport xgboost as xgb\nfrom entities import search_algorithm\nfrom sklearn.metrics import f1_score\n\n\nimport joblib\n\ndef uc01_hpo_train(feature_df):\n\n    decimal_columns = [ field.name for field in feature_df.schema.fields\n            if isinstance(field.datatype, numeric_types)]\n\n    for column_name in decimal_columns:\n        feature_df = feature_df.with_column(\n            column_name,\n            F.col(column_name).cast(\"float\")  # or \"float\" or DoubleType()\n            )\n    \n    oe_input_cols = ['GENDER', 'LOCATION', 'CUSTOMER_SEGMENT']\n    oe_output_cols = ['GENDER_OE', 'LOCATION_OE', 'CUSTOMER_SEGMENT_OE']\n\n    ss_input_numerical_cols = [\n        \"AGE\",\n        \"SENTIMENT_MIN_2\", \"SENTIMENT_MIN_3\", \"SENTIMENT_MIN_4\", \"SENTIMENT_AVG_2\",\n        \"SENTIMENT_AVG_3\", \"SENTIMENT_AVG_4\",\n        \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n        \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n    ]\n    \n    ss_output_numerical_cols = [col + \"_SS\" for col in ss_input_numerical_cols]\n    \n    label = ['CHURNED']\n    output_label = ['CHURNED_PRED']\n\n    input_columns = oe_input_cols + ss_input_numerical_cols + label\n    output_columns = oe_output_cols + ss_output_numerical_cols\n    \n    preprocessing_pipeline = sml_Pipeline(\n        steps=[ (\"OE\",\n                sml_OrdinalEncoder(\n                    input_cols=oe_input_cols,\n                    output_cols=oe_output_cols,\n                    drop_input_cols = True)),\n            (\"SS\",\n                sml_StandardScaler(\n                    input_cols=ss_input_numerical_cols,\n                    output_cols=ss_output_numerical_cols,\n                    drop_input_cols = True))\n            ])            \n\n    \n    train_df, testing_df = feature_df.select(input_columns).random_split(weights=[0.8, 0.2], seed=111)\n    \n    PIPELINE_FILE = f'/tmp/preprocessing_pipeline.joblib'\n    joblib.dump(preprocessing_pipeline, PIPELINE_FILE) # We are just pickling it locally first\n    training_spdf = preprocessing_pipeline.fit(train_df).transform(train_df)\n    testing_spdf = preprocessing_pipeline.fit(testing_df).transform(testing_df)\n\n    session.file.put(PIPELINE_FILE, \"@ML_STAGE\", overwrite=True)\n\n    dataset_map = {\n        \"x_train\": DataConnector.from_dataframe(training_spdf.drop(\"CHURNED\")),\n        \"y_train\": DataConnector.from_dataframe(training_spdf.select(\"CHURNED\")),\n        \"x_test\": DataConnector.from_dataframe(testing_spdf.drop(\"CHURNED\")),\n        \"y_test\": DataConnector.from_dataframe(testing_spdf.select(\"CHURNED\")),\n    }\n\n    def train_func():\n        tuner_context = get_tuner_context()\n        config = tuner_context.get_hyper_params()\n        dm = tuner_context.get_dataset_map()\n        model = xgb.XGBClassifier(\n   #         **{k: int(v) if k != \"learning_rate\" else v for k, v in config.items()},\n            random_state=42,\n        )\n        model.fit(dm[\"x_train\"].to_pandas(), dm[\"y_train\"].to_pandas())\n        test_f1_score = f1_score(\n            dm[\"y_test\"].to_pandas(), model.predict(dm[\"x_test\"].to_pandas())\n        )\n        train_f1_score = f1_score(\n            dm[\"y_train\"].to_pandas(), model.predict(dm[\"x_train\"].to_pandas())\n        )\n         \n        tuner_context.report(metrics={\"test_f1_score\": test_f1_score,\n                                     \"train_f1_score\": train_f1_score}, model=model)\n\n    \n    tuner = tune.Tuner(\n        train_func=train_func,\n        search_space={\n            \"n_estimators\": tune.uniform(50, 200),\n            \"max_depth\": tune.uniform(3, 10),\n            \"learning_rate\": tune.uniform(0.01, 0.3),\n        },\n        tuner_config=tune.TunerConfig(\n            metric=\"test_f1_score\",\n            mode=\"max\",\n            search_alg=search_algorithm.BayesOpt(),\n            num_trials=2,\n            max_concurrent_trials=1,\n        ),\n    )\n\n    tuner_results = tuner.run(dataset_map=dataset_map)\n    \n    return {'MODEL': tuner_results.best_model,\n            'train_f1_score': tuner_results.best_result['train_f1_score'] ,\n            'test_f1_score': tuner_results.best_result['test_f1_score'] \n           }\n    \n  ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e2e3a9f-4058-4369-a06c-e1c2791e500f",
   "metadata": {
    "language": "python",
    "name": "cell49"
   },
   "outputs": [],
   "source": "hpo_trained_model_results = uc01_hpo_train(training_dataset_sdf)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6281f46-e253-457c-ae1c-a20b9a097a87",
   "metadata": {
    "language": "python",
    "name": "cell50"
   },
   "outputs": [],
   "source": "hpo_trained_model_results\nprint (hpo_trained_model_results[\"test_f1_score\"][0])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "393d6808-03ad-4d52-ac33-80761b4510eb",
   "metadata": {
    "name": "cell52",
    "collapsed": false
   },
   "source": "Lets see what is the performance on the validation data. As we have separate pipeline transormations, we are going to define a inference function that perform the transformations first"
  },
  {
   "cell_type": "code",
   "id": "37e21da3-30fc-462c-925a-1562ea172490",
   "metadata": {
    "language": "python",
    "name": "def_inference_hpo"
   },
   "outputs": [],
   "source": "def inference_hpo (session, model, df):\n\n    oe_input_cols = ['GENDER', 'LOCATION', 'CUSTOMER_SEGMENT']\n    oe_output_cols = ['GENDER_OE', 'LOCATION_OE', 'CUSTOMER_SEGMENT_OE']\n\n    ss_input_numerical_cols = [\n        \"AGE\",\n        \"SENTIMENT_MIN_2\", \"SENTIMENT_MIN_3\", \"SENTIMENT_MIN_4\", \"SENTIMENT_AVG_2\",\n        \"SENTIMENT_AVG_3\", \"SENTIMENT_AVG_4\",\n        \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n        \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n    ]\n    \n    ss_output_numerical_cols = [col + \"_SS\" for col in ss_input_numerical_cols]\n    \n    label = ['CHURNED']\n    output_label = ['CHURNED_PRED']\n\n    input_columns = oe_input_cols + ss_input_numerical_cols + label\n    output_columns = oe_output_cols + ss_output_numerical_cols\n\n    \n    session.file.get(\"@ML_STAGE/preprocessing_pipeline.joblib.gz\", '/tmp')\n\n    pipeline_file_name = f'/tmp/preprocessing_pipeline.joblib'\n\n    preprocesing_pipeline = joblib.load(pipeline_file_name)\n\n    df = df.select(input_columns)\n    \n    preprocessed_df = preprocesing_pipeline.fit(df).transform(df)\n\n\n    dm = {\n        \"x_val\": DataConnector.from_dataframe(preprocessed_df.drop(\"CHURNED\")),\n        \"y_val\": DataConnector.from_dataframe(preprocessed_df.select(\"CHURNED\")),\n    }\n    \n    val_f1_score = f1_score(\n            dm[\"y_val\"].to_pandas(), model.predict(dm[\"x_val\"].to_pandas())\n    )\n\n    return val_f1_score\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10227c4b-317c-4406-ad82-976e709768da",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "inference_hpo (session, hpo_trained_model_results[\"MODEL\"], testing_dataset_sdf)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d06c78e9-c2b5-4f8f-a515-2b83d6ac4b15",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "## Distriburted Modeling Training\nNow let's explore how we can run Distributed Model Training for XGBoost"
  },
  {
   "cell_type": "code",
   "id": "4dcbed2c-7ce6-47fc-96cf-0a92d2b2175a",
   "metadata": {
    "language": "python",
    "name": "def_dist_train",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.preprocessing import StandardScaler as sml_StandardScaler\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder as sml_OrdinalEncoder\nfrom snowflake.ml.modeling.pipeline import Pipeline as sml_Pipeline\nfrom snowflake.ml.modeling.distributors.xgboost.xgboost_estimator import XGBEstimator, XGBScalingConfig\nfrom snowflake.ml.data.data_connector import DataConnector\nfrom sklearn.metrics import f1_score\n\nimport joblib\n\n\nfrom snowflake.snowpark.types import DecimalType, FloatType, IntegerType, DoubleType, LongType\nnumeric_types = (DecimalType, FloatType, IntegerType, DoubleType, LongType)\n\n\ndef uc01_dist_train(feature_df):\n\n    decimal_columns = [ field.name for field in feature_df.schema.fields\n            if isinstance(field.datatype, numeric_types)]\n\n    for column_name in decimal_columns:\n        feature_df = feature_df.with_column(\n            column_name,\n            F.col(column_name).cast(\"float\")  # or \"float\" or DoubleType()\n            )\n    \n    oe_input_cols = ['GENDER', 'LOCATION', 'CUSTOMER_SEGMENT']\n    oe_output_cols = ['GENDER_OE', 'LOCATION_OE', 'CUSTOMER_SEGMENT_OE']\n\n    ss_input_numerical_cols = [\n        \"AGE\",\n        \"SENTIMENT_MIN_2\", \"SENTIMENT_MIN_3\", \"SENTIMENT_MIN_4\", \"SENTIMENT_AVG_2\",\n        \"SENTIMENT_AVG_3\", \"SENTIMENT_AVG_4\",\n        \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n        \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n    ]\n    \n    ss_output_numerical_cols = [col + \"_SS\" for col in ss_input_numerical_cols]\n    \n    label = ['CHURNED']\n    output_label = ['CHURNED_PRED']\n\n    input_columns = oe_input_cols + ss_input_numerical_cols + label\n    output_columns = oe_output_cols + ss_output_numerical_cols\n    \n    preprocessing_pipeline = sml_Pipeline(\n        steps=[ (\"OE\",\n                sml_OrdinalEncoder(\n                    input_cols=oe_input_cols,\n                    output_cols=oe_output_cols,\n                    drop_input_cols = True)),\n            (\"SS\",\n                sml_StandardScaler(\n                    input_cols=ss_input_numerical_cols,\n                    output_cols=ss_output_numerical_cols,\n                    drop_input_cols = True))\n            ])            \n\n    \n    train_df, testing_df = feature_df.select(input_columns).random_split(weights=[0.8, 0.2], seed=111)\n\n    \n    PIPELINE_FILE = f'/tmp/preprocessing_pipeline.joblib'\n    joblib.dump(preprocessing_pipeline, PIPELINE_FILE) # We are just pickling it locally first\n    training_spdf = preprocessing_pipeline.fit(train_df).transform(train_df)\n    testing_spdf = preprocessing_pipeline.fit(testing_df).transform(testing_df)\n\n    \n    session.file.put(PIPELINE_FILE, \"@ML_STAGE\", overwrite=True)   \n\n    dm = {\n        \"x_train\": DataConnector.from_dataframe(training_spdf),\n        \"y_train\": DataConnector.from_dataframe(training_spdf.select(\"CHURNED\")),\n        \"x_test\": DataConnector.from_dataframe(testing_spdf),\n        \"y_test\": DataConnector.from_dataframe(testing_spdf.select(\"CHURNED\")),\n    }\n\n    dist_gxb = XGBEstimator(\n        objective = 'binary:logistic'\n    )\n\n    dist_gxb.fit(dm[\"x_train\"],\n                 input_cols=output_columns + label,\n                 label_col=label[0]) \n\n\n    predict_test = dist_gxb.predict(dm[\"x_test\"].to_pandas())\n    predict_train = dist_gxb.predict(dm[\"x_train\"].to_pandas())\n\n    y_predict_test = (predict_test > 0.5).astype(int)\n    y_predict_train = (predict_train > 0.5).astype(int)\n    \n    test_f1_score = f1_score(\n            dm[\"y_test\"].to_pandas(), y_predict_test\n    )\n\n    train_f1_score = f1_score(\n            dm[\"y_train\"].to_pandas(), y_predict_train\n    )\n\n    \n    return {'MODEL': dist_gxb,\n            'train_f1_score': train_f1_score ,\n            'test_f1_score': test_f1_score\n           }\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65fcb406-1551-4484-8f75-2562a742f89a",
   "metadata": {
    "language": "python",
    "name": "cell57"
   },
   "outputs": [],
   "source": "dist_model_trained = uc01_dist_train(training_dataset_sdf)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b08c455a-9cb7-44ee-bc9c-b7cdb21653b8",
   "metadata": {
    "language": "python",
    "name": "cell59"
   },
   "outputs": [],
   "source": "dist_model_trained",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b2b18166-a2ee-41bb-a47f-b5cd9506b2d8",
   "metadata": {
    "name": "cell29",
    "collapsed": false
   },
   "source": "## Model Registry\n\nRegister some of the models we have being using"
  },
  {
   "cell_type": "code",
   "id": "240df13d-551e-4feb-939f-8d9600160c27",
   "metadata": {
    "language": "python",
    "name": "log_base_model",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.model import type_hints\n\nmodel_logged = mr.log_model(model= trained_model['MODEL'],\n                model_name= \"ChurnDetector\",\n                version_name= \"v0\",\n                #conda_dependencies=[\"snowflake-ml-python\"],\n                sample_input_data = training_dataset_sdf.limit(100),\n                #options={\"relax_version\": False, \"enable_explainability\": True},\n                task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n                comment=\"Model to detect what customers will not buy again\"\n                )\n\nmodel_logged.set_metric(metric_name=\"train_f1_score\", value=trained_model['train_f1_score'])\nmodel_logged.set_metric(metric_name=\"test_f1_score\", value=trained_model['train_f1_score'])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e079edf7-09d8-4196-8165-0b829d3900de",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "from snowflake.ml.model import type_hints\n\noe_input_cols = ['GENDER', 'LOCATION', 'CUSTOMER_SEGMENT']\n\nss_input_numerical_cols = [\n    \"AGE\",\n    \"SENTIMENT_MIN_2\", \"SENTIMENT_MIN_3\", \"SENTIMENT_MIN_4\", \"SENTIMENT_AVG_2\",\n    \"SENTIMENT_AVG_3\", \"SENTIMENT_AVG_4\",\n    \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n    \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n]\ncolumns = oe_input_cols + ss_input_numerical_cols\n\nmodel_oss_logged = mr.log_model(model= trained_oss_model['MODEL'],\n                model_name= \"ChurnDetector\",\n                version_name= \"base\",\n                conda_dependencies=[\"snowflake-ml-python\", \"xgboost\", \"scikit-learn\"],\n                sample_input_data = training_dataset_sdf.select(columns).limit(100),\n                #options={\"relax_version\": False, \"enable_explainability\": True},\n                task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n                comment=\"Model to detect what customers will not buy again\"\n                )\n\nmodel_oss_logged.set_metric(metric_name=\"train_f1_score\", value=trained_oss_model['train_f1_score'])\nmodel_oss_logged.set_metric(metric_name=\"test_f1_score\", value=trained_oss_model['train_f1_score'])\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c8e789b-bd07-4717-98e0-76404de8be96",
   "metadata": {
    "language": "python",
    "name": "cell69"
   },
   "outputs": [],
   "source": "sc = session.get_current_schema()\n\nsession.use_schema(mr_schema)\n\nsession.sql('ALTER MODEL ChurnDetector SET DEFAULT_VERSION = base;').collect()\n\nsession.use_schema(sc)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2189e14b-0bf9-42d3-be7f-899a78b2b519",
   "metadata": {
    "language": "sql",
    "name": "cell33",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "---These are the tables that will be used for model monitoring:\n--- Cleanup to start fresh\n\ndrop table if exists customer_churn_baseline_predicted;\ndrop table if exists customer_churn_predicted;\ndrop table if exists customer_churn_predicted_prod;\ndrop table if exists customer_churn_predicted_proba;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46712ab3-d663-4af4-9a6a-6f0c90d3633b",
   "metadata": {
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": "# Create the predictions tables. The one for the baseline and the one where we keep adding predicitions\n\ntables = [\"customer_churn_baseline_predicted\", \"CUSTOMER_CHURN_PREDICTED_PROD2\"]\n\nfor table in tables:\n    sql_cmd = f\"\"\"       \n        create or replace TABLE {table} (\n        \tCUSTOMER_ID VARCHAR(16777216),\n        \tTIMESTAMP TIMESTAMP_NTZ(9),\n        \tGENDER VARCHAR(16777216),\n        \tLOCATION VARCHAR(16777216),\n        \tCUSTOMER_SEGMENT VARCHAR(16777216),\n        \tLAST_PURCHASE_DATE DATE,\n        \tNEXT_TRANSACTION_DATE DATE,\n        \tAGE FLOAT,\n        \tSENTIMENT_MIN_2 FLOAT,\n        \tSENTIMENT_MIN_3 FLOAT,\n        \tSENTIMENT_MIN_4 FLOAT,\n        \tSENTIMENT_AVG_2 FLOAT,\n        \tSENTIMENT_AVG_3 FLOAT,\n        \tSENTIMENT_AVG_4 FLOAT,\n        \tSUM_TOTAL_AMOUNT_PAST_7D FLOAT,\n        \tSUM_TOTAL_AMOUNT_PAST_1MM FLOAT,\n        \tSUM_TOTAL_AMOUNT_PAST_2MM FLOAT,\n        \tSUM_TOTAL_AMOUNT_PAST_3MM FLOAT,\n        \tCOUNT_ORDERS_PAST_7D FLOAT,\n        \tCOUNT_ORDERS_PAST_1MM FLOAT,\n        \tCOUNT_ORDERS_PAST_2MM FLOAT,\n        \tCOUNT_ORDERS_PAST_3MM FLOAT,\n        \tDAYS_SINCE_LAST_PURCHASE FLOAT,\n        \tCHURNED FLOAT,\n        \tCHURNED_PRED_PROD FLOAT,\n            CHURNED_PRED_BASE FLOAT,\n            CHURNED_PRED_RETRAIN FLOAT,\n            CHURNED_PRED_PROBABILITY FLOAT,\n        \tVERSION_NAME VARCHAR(50)\n        );\n        \"\"\"\n    session.sql(sql_cmd).collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4a47af08-3a2f-42ea-9d9e-8a7886bd92f8",
   "metadata": {
    "language": "python",
    "name": "def_inference_oss2"
   },
   "outputs": [],
   "source": "def inference_oss (model, inference_df, output_table, col_name, is_prod):\n    \n    predictions = model.run(inference_df, function_name=\"predict\")\n\n    predictions = predictions.select([F.col(c).alias(c.replace('\"', '')) for c in predictions.columns])\n\n    predictions_out = predictions.rename(\"output_feature_0\", col_name)\n\n    model_version = model.version_name\n    predictions_out = predictions_out.with_column(\"version_name\", F.lit(model_version))\n    \n    predictions_out.write.mode(\"overwrite\").save_as_table('temp_predictions')\n\n    # Get full schema of output table\n    output_columns = [field.name for field in session.table(output_table).schema]\n\n    # Insert clause: match columns from temp_predictions, else use NULL\n    insert_columns = \", \".join(output_columns)\n    insert_values = \", \".join([\n        f\"t.{col}\" if col in predictions_out.columns else \"NULL\" for col in output_columns\n    ])\n\n   # Merge statement: update col_name if record exists, insert all columns if not\n    merge_statement = f\"\"\"\n        MERGE INTO {output_table} o\n        USING temp_predictions t\n        ON o.CUSTOMER_ID = t.CUSTOMER_ID AND o.TIMESTAMP = t.TIMESTAMP\n        WHEN MATCHED THEN\n            UPDATE SET o.{col_name} = t.{col_name},\n                       o.version_name = t.version_name\n        WHEN NOT MATCHED THEN\n            INSERT ({insert_columns})\n            VALUES ({insert_values})\n    \"\"\"\n    \n    session.sql(merge_statement).collect()    \n    print (f'Predictions added to table {output_table} ')\n\n    ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9302eab-b2a7-4017-8a43-f712f2b0f880",
   "metadata": {
    "language": "sql",
    "name": "cell74"
   },
   "outputs": [],
   "source": "select * from CUSTOMER_CHURN_PREDICTED_PROD2 limit 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1dccf1c5-1b84-489f-8f9a-dacb9e0aeb72",
   "metadata": {
    "language": "python",
    "name": "cell75"
   },
   "outputs": [],
   "source": "## Feed the predictiosn table with the data we already have.\n\nbase_model = mr.get_model(\"ChurnDetector\").version(\"base\")\n\ninference_oss(base_model, training_dataset_sdf, 'customer_churn_baseline_predicted', 'CHURNED_PRED_BASE', True)\ninference_oss(base_model, testing_dataset_sdf, 'CUSTOMER_CHURN_PREDICTED_PROD2', 'CHURNED_PRED_PROD', True)\ninference_oss(base_model, testing_dataset_sdf, 'CUSTOMER_CHURN_PREDICTED_PROD2', 'CHURNED_PRED_BASE', True)\ninference_oss(base_model, testing_dataset_sdf, 'CUSTOMER_CHURN_PREDICTED_PROD2', 'CHURNED_PRED_RETRAIN', True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "efe3c20a-cf50-40a9-8f63-982ff9331791",
   "metadata": {
    "language": "sql",
    "name": "cell76"
   },
   "outputs": [],
   "source": "select * from CUSTOMER_CHURN_PREDICTED_PROD2 limit 5;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ecd102cf-d636-40d2-a307-a9842bae79e4",
   "metadata": {
    "name": "cell35",
    "collapsed": false
   },
   "source": "## Model Monitors"
  },
  {
   "cell_type": "code",
   "id": "725401ac-f5f6-469d-a3d4-797b5b3c2ca2",
   "metadata": {
    "language": "python",
    "name": "def_monitor_base",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "## Monitor definition for the predictions done on customer_churn_predicted table\n## Associated to first base model \n\nsc = session.get_current_schema()\n\nsession.use_schema(mr_schema)\n\ncmd_sql = f\"\"\"\nCREATE OR REPLACE MODEL MONITOR Monitor_ChurnDetector_Base\nWITH\n    MODEL=ChurnDetector\n    VERSION=base\n    FUNCTION=predict\n    SOURCE={sc}.CUSTOMER_CHURN_PREDICTED_PROD2\n    BASELINE={sc}.customer_churn_baseline_predicted\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(CHURNED_PRED_BASE)  \n    ACTUAL_CLASS_COLUMNS=(CHURNED)\n    ID_COLUMNS=(CUSTOMER_ID)\n    WAREHOUSE=COMPUTE_WH\n    REFRESH_INTERVAL='1 min'\n    AGGREGATION_WINDOW='1 day';\n\"\"\"\n\nsession.sql(cmd_sql).collect()\n\nsession.use_schema(sc)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f80729e4-15a4-46de-9cae-0fb0bd19f122",
   "metadata": {
    "language": "python",
    "name": "log_monitor_prod"
   },
   "outputs": [],
   "source": "fake_prod_model = mr.get_model(\"ChurnDetector\").version(\"base\")\n\nfake_prod_logged = mr.log_model(model= fake_prod_model,\n                        model_name= \"ChurnDetector\",\n                        version_name= \"PRODMONITOR\",\n                        #conda_dependencies=[\"snowflake-ml-python\"],\n                        sample_input_data = training_dataset_sdf.select(columns).limit(100),\n                        #options={\"relax_version\": False, \"enable_explainability\": True},\n                        task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n                        comment=\"Model to detect what customers will not buy again\"\n                        )\n\n\nsc = session.get_current_schema()\n\nsession.use_schema(mr_schema)\n\ncmd_sql = f\"\"\"\nCREATE OR REPLACE MODEL MONITOR Monitor_ChurnDetector_Prod\nWITH\n    MODEL=ChurnDetector\n    VERSION=PRODMONITOR\n    FUNCTION=predict\n    SOURCE={sc}.CUSTOMER_CHURN_PREDICTED_PROD2\n    BASELINE={sc}.customer_churn_baseline_predicted\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(CHURNED_PRED_PROD)  \n    ACTUAL_CLASS_COLUMNS=(CHURNED)\n    ID_COLUMNS=(CUSTOMER_ID)\n    WAREHOUSE=COMPUTE_WH\n    REFRESH_INTERVAL='1 min'\n    AGGREGATION_WINDOW='1 day';\n\"\"\"\n\nsession.sql(cmd_sql).collect()\n\nsession.use_schema(sc)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2143c857-8ca1-4baf-ad19-b12bb0a7b6ba",
   "metadata": {
    "language": "python",
    "name": "log_monitor_retrain"
   },
   "outputs": [],
   "source": "fake_retrain_model = mr.get_model(\"ChurnDetector\").version(\"base\")\n\nfake_retrain_logged = mr.log_model(model= fake_retrain_model,\n                        model_name= \"ChurnDetector\",\n                        version_name= \"RETRAIN\",\n                        #conda_dependencies=[\"snowflake-ml-python\"],\n                        sample_input_data = training_dataset_sdf.select(columns).limit(100),\n                        #options={\"relax_version\": False, \"enable_explainability\": True},\n                        task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n                        comment=\"Model to detect what customers will not buy again\"\n                        )\n\nsc = session.get_current_schema()\n\nsession.use_schema(mr_schema)\n\ncmd_sql = f\"\"\"\nCREATE OR REPLACE MODEL MONITOR Monitor_ChurnDetector_Retrain\nWITH\n    MODEL=ChurnDetector\n    VERSION=RETRAIN\n    FUNCTION=predict\n    SOURCE={sc}.CUSTOMER_CHURN_PREDICTED_PROD2\n    BASELINE={sc}.customer_churn_baseline_predicted\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(CHURNED_PRED_RETRAIN)  \n    ACTUAL_CLASS_COLUMNS=(CHURNED)\n    ID_COLUMNS=(CUSTOMER_ID)\n    WAREHOUSE=COMPUTE_WH\n    REFRESH_INTERVAL='1 min'\n    AGGREGATION_WINDOW='1 day';\n\"\"\"\n\nsession.sql(cmd_sql).collect()\n\nsession.use_schema(sc)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "843d2fa8-4f3f-45d6-a262-2265c6bbba64",
   "metadata": {
    "language": "python",
    "name": "def_get_f1_score"
   },
   "outputs": [],
   "source": "def get_last_labeled_f1_score(model_monitor):\n    \n    session.sql(f'use schema {SCHEMA} ').collect()\n    \n    timestamps = session.table(table_features_labeled).select(\"TIMESTAMP\").distinct().sort(\"TIMESTAMP\").collect()\n    \n    timestamp_1 = timestamps[-2][\"TIMESTAMP\"]\n        \n    session.sql(f'use schema {mr_schema} ').collect()\n \n    sql_cmd = f\"\"\"\n        SELECT * FROM TABLE(MODEL_MONITOR_PERFORMANCE_METRIC(\n        '{model_monitor}', 'F1_SCORE', '1 DAY', '{timestamp_1}', '{timestamp_1}'));\n    \"\"\"\n    output = session.sql(sql_cmd).collect()\n    \n    if not output or output[0][\"METRIC_VALUE\"] is None:\n        print(\"No metric data returned for given timestamp. Probably too early.\")\n        session.sql(f'use schema {SCHEMA} ').collect()\n\n        return 1  # If we have no info, let's do not force re-train for now...\n        \n    metric = output[0][\"METRIC_VALUE\"]\n    \n    session.sql(f'use schema {SCHEMA} ').collect()\n\n    return metric\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "131cc8d1-0a33-414c-8ff9-74ba9a10528f",
   "metadata": {
    "language": "python",
    "name": "cell65"
   },
   "outputs": [],
   "source": "from snowflake.ml import dataset\n\ndef get_inference_dataset():\n\n    ts_inference_tb = session.table(table_features_labeled).select(\"TIMESTAMP\").distinct().sort(\"TIMESTAMP\").collect()\n    \n    ts_inference = ts_inference_tb[-2][\"TIMESTAMP\"]\n\n    date_name = \"v_\" + str(ts_inference.date()).replace(\"-\", \"_\")\n    ds_name = f'{fs_schema}.UC01_DATASET_{date_name}'\n\n    inference_dataset = dataset.load_dataset(session, ds_name, 'v1')\n\n    inference_dataset_sdf = inference_dataset.read.to_snowpark_dataframe()\n\n    return inference_dataset_sdf\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a34655f2-dc2b-499c-84c1-8713d30bba91",
   "metadata": {
    "language": "python",
    "name": "cell83"
   },
   "outputs": [],
   "source": "def set_default_model(name):\n\n    sc = session.get_current_schema()\n    session.use_schema(mr_schema)\n\n    session.sql(f'ALTER MODEL ChurnDetector SET DEFAULT_VERSION = {name};').collect()\n\n    session.use_schema(sc)\n\n    print (f\"Setting default PROD Model Version to {name} \")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50e25baa-a609-4d93-9816-b29bde4d88c4",
   "metadata": {
    "language": "python",
    "name": "def_process_one_month"
   },
   "outputs": [],
   "source": "#Loading all other moths, predict using first model and re-train the model\n\nfrom snowflake.snowpark import functions as F\nfrom datetime import datetime\nimport time\n\nstage_name = \"DATASET.CSV\"\nchurn_window = 30 ## This is the value we define as churn\ntable_features = 'churn_baseline'\ntable_features_labeled = 'churn_baseline_labeled'\n\n\ndef process_one_month (db, sc, fs, fv, mr, columns):\n    \n    t1 = datetime.now()\n\n    # STEP 1: COPY NEXT SALES AND FEEDBACK MONTH\n    print (\"######################################################################\")\n    if (copy_next_file (session, db, sc) == False):\n        print ('No more data to be ingested')\n        return False\n    \n    # STEP 2: PROCESS THE FEEDBACK RECEIVED\n    print (\"Processing Sentiment\")\n    process_sentiment()\n    \n    t2 = datetime.now()\n    print (f'STEP 1, 2: File ingested and sentiment calculated: {t2 - t1}')\n    \n    # STEP 3: CALCULATE FEATURES FOR LATEST TIMESTAMP (DATA JUST INGESTED)\n    sales_df = session.table(\"sales\") \n    \n    #calculate features for the latest transaction timestamp\n    latest_transaction = sales_df.select(F.max(F.col(\"transaction_date\"))).collect()[0][0]\n    \n    session.call('UTILS.uc01_feature_engineering_sproc', db, sc, latest_transaction, table_features)\n    \n    t3 = datetime.now()\n    print (f'STEP 3: Calculated features for timestamp : {latest_transaction}, {t3 - t2} ')\n       \n    # STEP 4: SET RIGHT CHURN LABEL BASED ON NEW SALES DATA\n    \n    session.call('UTILS.uc_01_label_churn_sproc', db, sc, table_features, table_features_labeled, 30 )\n\n    t4 = datetime.now()\n    print (f'STEP 4: Added real labels for churn  window: {churn_window}, {t4 - t3}  ')\n\n    # STEP 5: INFERENCE ON THE NEW DATA. FIRST LETS USE THE BASE MODEL\n    \n    # Get the latest timestamp from the features table\n    churn_baseline_labeled_df = session.table(table_features_labeled)\n                      \n    latest_feature_timestamp = churn_baseline_labeled_df.select(F.max(F.col(\"timestamp\"))).collect()[0][0]\n\n    print (f'latest feature timestamp:{latest_feature_timestamp} ')\n\n    date_name = \"v_\" + str(latest_feature_timestamp.date()).replace(\"-\", \"_\")\n    ds_name = f'UC01_DATASET_{date_name}'\n    \n    new_dataset_sdf = fs_generate_dataset(fs, fv, ds_name, latest_feature_timestamp)\n\n    t5 = datetime.now()\n    print (f'STEP 5: Data set generated for timestamp: {latest_feature_timestamp}, {t5 - t4} ')\n\n    # STEP 6: RUN INFERENCE USING THE BASE MODEL ON NEW DATA\n    ## Run inference using the first model training just for demo purposes so we can compare using the Monitors\n    \n    base_model = mr.get_model(\"ChurnDetector\").version(\"base\")\n    \n    #After inference, let's update the previous labels as now we know if there were more sales    \n    inference_oss(base_model, new_dataset_sdf, 'CUSTOMER_CHURN_PREDICTED_PROD2', 'CHURNED_PRED_BASE', False)\n    \n    # Update the predictions for the Model Monitor\n    session.call('UTILS.uc01_update_label_churn_sproc', db, sc, 'CUSTOMER_CHURN_PREDICTED_PROD2', churn_window )\n\n    t6 = datetime.now()\n    print (f'STEP 6: Inference run using Base model {t6 - t5} ')\n\n\n    # Wait at least 1 min for the monitor to refresh statistics\n    print ('--- Waiting 70 seconds for Monitors to be updated')\n    time.sleep(70)\n    \n    t6 = datetime.now()\n   \n    # STEP 7: NOW THAT WE HAVE THE LABELS OF THE PREVIOUS PERIOD, WE CAN CHECK THE MODEL MONITOR FOR PERFORMANCE\n\n    f1_score_last_period_base = get_last_labeled_f1_score('Monitor_ChurnDetector_Base')\n    f1_score_last_period_prod = get_last_labeled_f1_score('Monitor_ChurnDetector_Prod')\n    f1_score_last_period_retrain = 0 # do not use unless we have to retrain\n    \n    t7 = datetime.now()\n    print (f'Got Monitor Statistics,   {t7 - t6}')\n    \n    print (f\"BASE Model: f1_score for previous period is: {f1_score_last_period_base} \")\n    print (f\"PROD Model: f1_score for previous period is: {f1_score_last_period_prod} \")\n\n\n    # STEP 7: DECIDE IF WE TRAIN ANOTHER MODEL\n\n    # If the base monitor is less than \n    if (f1_score_last_period_prod < 0.8):\n    \n        print ('Training new model because performance drop')\n        \n        new_trained_model = uc01_oss_train(new_dataset_sdf)\n    \n        print (\"Inferencing with the re-trained model\")\n    \n        model_logged = mr.log_model(model= new_trained_model['MODEL'],\n                        model_name= \"ChurnDetector\",\n                        version_name= date_name,\n                        conda_dependencies=[\"snowflake-ml-python\", \"xgboost\", \"scikit-learn\"],\n                        sample_input_data = new_dataset_sdf.select(columns).limit(100),\n                        #options={\"relax_version\": False, \"enable_explainability\": True},\n                        task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n                        comment=\"Model to detect what customers will not buy again\"\n                        )\n        \n        model_logged.set_metric(metric_name=\"train_f1_score\", value=new_trained_model['train_f1_score'])\n        model_logged.set_metric(metric_name=\"test_f1_score\", value=new_trained_model['test_f1_score'])\n\n        t8 = datetime.now()\n\n        print (f'STEP 7(Op): Trained new model {date_name},  {t8 - t7} ')\n        print (f\"train_f1_score: {new_trained_model['train_f1_score']} \")\n        print (f\"test_f1_score: {new_trained_model['test_f1_score']} \")\n    \n        # We have to see how this model works on the previous dataset ()\n\n        prev_inference_dataset_sdf = get_inference_dataset()\n        \n        trained_model = mr.get_model(\"ChurnDetector\").version(date_name)\n\n        inference_oss(trained_model, prev_inference_dataset_sdf, 'CUSTOMER_CHURN_PREDICTED_PROD2', 'CHURNED_PRED_RETRAIN', True)     \n        print ('--- Waiting 70 seconds for Monitors to be updated')\n        time.sleep(70)\n        \n        f1_score_last_period_retrain = get_last_labeled_f1_score('Monitor_ChurnDetector_Retrain')\n        print (f\"NEW Model {date_name} : f1_score for previous period is: {f1_score_last_period_retrain} \")\n\n    else:\n        print (\"STEP 7: No need for training a new model\")\n\n    ## STEP 8: Decide what is the model that best performance on the :\n\n    # If base model is better than Prod and Retrain model we set it as default\n\n    if (f1_score_last_period_base > f1_score_last_period_prod) & (f1_score_last_period_base > f1_score_last_period_retrain):\n        set_default_model ('base')\n    ## if new model is better than current and base, set it as default\n    elif (f1_score_last_period_retrain > f1_score_last_period_base) & (f1_score_last_period_retrain > f1_score_last_period_prod):\n        set_default_model (date_name)\n\n    t9 = datetime.now()\n\n    # Get whatever is the PROD default model to run next inference on the new arrived data\n    \n    prod_model = mr.get_model(\"ChurnDetector\").default\n\n    print (f\"Running Inference for Prod Label with model {prod_model.version_name}\")\n    \n    inference_oss(prod_model, new_dataset_sdf, 'CUSTOMER_CHURN_PREDICTED_PROD2', 'CHURNED_PRED_PROD', True)\n    #After inference, let's update the previous labels as now we know if there were more sales\n    #session.call('UTILS.uc01_update_label_churn_sproc', db, sc, 'CUSTOMER_CHURN_PREDICTED_PROD2', churn_window )\n\n    t10 = datetime.now()\n    print (f'Inference done using {prod_model.version_name} Model, {t10 - t9}')\n\n    return True\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64a2b188-f8e6-4475-ad10-ffa70e7991bf",
   "metadata": {
    "language": "python",
    "name": "cell37",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# columns were defined at the very beginnign and are the columns we are using for training. \n\ndb = session.get_current_database()\nsc = session.get_current_schema()\n\nprint (f'database: {db}, schema: {sc}')\nprocess_one_month(db, sc, fs, fv_uc01_preprocess, mr,  columns)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d93071cd-3911-49f7-b771-d146ac913e8a",
   "metadata": {
    "language": "sql",
    "name": "cell71"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b145327d-9836-4ad0-8cb2-1ce78b166258",
   "metadata": {
    "language": "python",
    "name": "Process_all_files",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\ndb = session.get_current_database()\nsc = session.get_current_schema()\n\nprint (f'database: {db}, schema: {sc}')\n\nret = True\n\nwhile (ret == True):\n    ret = process_one_month(db, sc, fs, fv_uc01_preprocess, mr,  columns)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61c985eb-e481-485d-b5af-af18ac3cd046",
   "metadata": {
    "language": "sql",
    "name": "check_predictions"
   },
   "outputs": [],
   "source": "select DISTINCT(TIMESTAMP) as TS, VERSION_NAME from CUSTOMER_CHURN_PREDICTED_PROD2\norder by TS ASC;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b883ca81-ef7f-4218-8639-02eaea5523a0",
   "metadata": {
    "name": "cell72",
    "collapsed": false
   },
   "source": "### Add Data Drift\nWe are going to add new sales where we have changes in locations and customer_segements\n"
  },
  {
   "cell_type": "code",
   "id": "c5149f72-6688-4f33-8a1d-6a9751b48e17",
   "metadata": {
    "language": "python",
    "name": "cell62"
   },
   "outputs": [],
   "source": "\nsql_cmd = f\"\"\"\n    COPY INTO {db}.{sc}.customers  \n            FROM @{stage_name}/new_customers.csv  \n            FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"')  \n            ON_ERROR = 'CONTINUE'; \n\"\"\"\n\nsession.sql(sql_cmd).collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0565926b-2291-4f4f-8ae2-8d9402831849",
   "metadata": {
    "language": "python",
    "name": "insert_new_files_drift"
   },
   "outputs": [],
   "source": "new_sales_month_files = get_year_month_files(session, stage_name, 'new_sales')\ninsert_files ('sales', db, sc, new_sales_month_files)\n\nnew_feedback_sentiment_month_files = get_year_month_files(session, stage_name, 'new_feedback_raw2')\ninsert_files ('feedback_raw',db, sc, new_feedback_sentiment_month_files)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5b3727e-851a-4a7a-a722-5cdf6c101bdc",
   "metadata": {
    "language": "python",
    "name": "cell70"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7c2a6d1-1f89-4cd8-a789-f1aa6d1f40f5",
   "metadata": {
    "language": "sql",
    "name": "cell68",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "select * from FILES_INGESTED where INGESTED = False;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f4dc6dbb-bdce-47b1-986c-ea0f51c109f4",
   "metadata": {
    "language": "python",
    "name": "ingested_sked_data"
   },
   "outputs": [],
   "source": "## Add the new files:\ndb = session.get_current_database()\nsc = session.get_current_schema()\n\nprint (f'database: {db}, schema: {sc}')\n\nret = True\n\nwhile (ret == True):\n    ret = process_one_month(db, sc, fs, fv_uc01_preprocess, mr,  columns)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66bc7b47-2910-4263-a8f3-1a79510f010f",
   "metadata": {
    "language": "sql",
    "name": "cell79"
   },
   "outputs": [],
   "source": "select DISTINCT(TIMESTAMP) as TS, VERSION_NAME from CUSTOMER_CHURN_PREDICTED_PROD2\norder by TS ASC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a2b55f1-79d6-4abe-a191-0a59de800e83",
   "metadata": {
    "language": "python",
    "name": "cell21",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "f1_score_last_period_base = get_last_labeled_f1_score('Monitor_ChurnDetector_Prod')\nprint (f1_score_last_period_base)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "941e27d4-8d86-4490-948f-ea55d4464e66",
   "metadata": {
    "name": "observability_metrics",
    "collapsed": false
   },
   "source": "## Observability Metrics\n\nWe have used observability above to detect performance drops but let's take a look here too. Also, you can use Snowsight to see the model performance and compare with others\n"
  },
  {
   "cell_type": "code",
   "id": "538d4c53-7d95-4e5c-abde-29b43cb422fe",
   "metadata": {
    "language": "python",
    "name": "cell78"
   },
   "outputs": [],
   "source": "def get_model_metric(model_monitor, metric):\n    \n    session.sql(f'use schema {SCHEMA} ').collect()\n\n    first_transaction = session.table('CUSTOMER_CHURN_PREDICTED_PROD2').select(F.min(F.col(\"TIMESTAMP\"))).collect()[0][0] \n    last_transaction = session.table('CUSTOMER_CHURN_PREDICTED_PROD2').select(F.max(F.col(\"TIMESTAMP\"))).collect()[0][0]\n\n    \n    print (f'Calculating {metric} from model {model_monitor} from timestamp: {first_transaction} to {last_transaction} ')\n    \n    session.sql(f'use schema {mr_schema} ').collect()\n \n    sql_cmd = f\"\"\"\n        SELECT * FROM TABLE(MODEL_MONITOR_PERFORMANCE_METRIC(\n        '{model_monitor}', '{metric}', '1 DAY', '{first_transaction}', '{last_transaction}'));\n    \"\"\"\n    output = session.sql(sql_cmd).collect()\n\n    session.sql(f'use schema {SCHEMA} ').collect()\n \n    return output",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1421996-6d88-4f42-9a52-f07be78fbd00",
   "metadata": {
    "language": "python",
    "name": "cell77"
   },
   "outputs": [],
   "source": "out = get_model_metric('Monitor_ChurnDetector_Prod', 'F1_SCORE')\nout",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a54ebfe8-e4d4-4d38-8870-8d84fcfaa346",
   "metadata": {
    "language": "python",
    "name": "cell81"
   },
   "outputs": [],
   "source": "\nout = get_model_metric('Monitor_ChurnDetector_Prod', 'CLASSIFICATION_ACCURACY')\nout\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7ce86ab-ca30-47d2-904b-a426219eb8b7",
   "metadata": {
    "language": "sql",
    "name": "cell82"
   },
   "outputs": [],
   "source": "select current_schema()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6bc2d696-e291-43f3-b7f4-592108ce0a80",
   "metadata": {
    "language": "sql",
    "name": "cell73",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\nCREATE or replace TABLE ALERTS_NOTIFICATION(\n    notification varchar (100),\n    created_at timestamp);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5afac3df-ce6c-4c2e-9868-423242a15ce5",
   "metadata": {
    "language": "python",
    "name": "def_create_alert"
   },
   "outputs": [],
   "source": "\ndef create_alert(model_monitor, metric, value):\n    \n    session.sql(f'use schema {mr_schema}').collect()\n\n    alert_name = f'{model_monitor}_{metric}_alert'\n\n    sql_cmd = f\"\"\"\n        create or replace ALERT {alert_name}\n        WAREHOUSE = COMPUTE_WH\n        SCHEDULE = '1 minute' --- Note this is just for this demo so I can see the alert live\n        IF (EXISTS (\n            SELECT METRIC_VALUE\n            FROM TABLE(\n                MODEL_MONITOR_PERFORMANCE_METRIC(\n                    '{model_monitor}', \n                    '{metric}', \n                    '1 DAY', \n                    (\n                        SELECT ts \n                        FROM (\n                            SELECT DISTINCT TIMESTAMP AS ts \n                            FROM {SCHEMA}.CUSTOMER_CHURN_PREDICTED_PROD2 \n                            ORDER BY ts DESC \n                            LIMIT 2\n                        ) AS sub \n                        ORDER BY ts \n                        LIMIT 1\n                    ) , \n                    (\n                        SELECT ts \n                        FROM (\n                            SELECT DISTINCT TIMESTAMP AS ts \n                            FROM {SCHEMA}.CUSTOMER_CHURN_PREDICTED_PROD2 \n                            ORDER BY ts DESC \n                            LIMIT 2\n                        ) AS sub \n                        ORDER BY ts \n                        LIMIT 1\n                    ) \n                )\n            ) where METRIC_VALUE < {value})\n         )\n         THEN\n            INSERT INTO {SCHEMA}.ALERTS_NOTIFICATION (notification, created_at) VALUES\n            ('{metric} performance dropped less than {value} on monitor {model_monitor} ', (SELECT CURRENT_TIMESTAMP));\n    \"\"\"\n\n    session.sql(sql_cmd).collect()\n\n    session.sql(f'alter ALERT {alert_name} RESUME').collect()\n    session.sql(f'execute ALERT {alert_name}').collect()\n\n    session.sql(f'use schema {SCHEMA} ').collect()\n\n    print (f'Created and Executed Alert {alert_name} ')\n\n    return alert_name\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8204cc9f-16c0-49be-a920-cfc4a9ceee03",
   "metadata": {
    "language": "python",
    "name": "cell85"
   },
   "outputs": [],
   "source": "\nalert_name = create_alert ('Monitor_ChurnDetector_Prod', 'F1_SCORE', 0.99)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dabff52c-778f-451a-b45b-abfdcfd6f12f",
   "metadata": {
    "language": "sql",
    "name": "cell84",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "select * from ALERTS_NOTIFICATION;\n        ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ed07767-a310-4353-bae6-4d99410071d3",
   "metadata": {
    "language": "python",
    "name": "drop_alert"
   },
   "outputs": [],
   "source": "session.sql(f'use schema {mr_schema}').collect()\n\nsession.sql(f'alter alert {alert_name} suspend ').collect()\n\nsession.sql(f'use schema {SCHEMA} ').collect()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "49c2507a-471f-46ff-9ba7-40870c854686",
   "metadata": {
    "name": "cell23",
    "collapsed": false
   },
   "source": "## Registering a Model within Snowpark Container Services for Online Inference"
  },
  {
   "cell_type": "code",
   "id": "cd41f9d9-fd6e-4a48-a8b3-c70572d75af5",
   "metadata": {
    "language": "sql",
    "name": "cell44",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\n CREATE COMPUTE POOL IF NOT EXISTS CPU_XS_INFERENCE2\n    MIN_NODES = 1\n    MAX_NODES = 2\n    INSTANCE_FAMILY = 'CPU_X64_S'\n    AUTO_RESUME = TRUE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dca48ae3-dec9-49ec-a0d4-8a9cab007ae7",
   "metadata": {
    "language": "sql",
    "name": "cell24"
   },
   "outputs": [],
   "source": "CREATE IMAGE REPOSITORY IF NOT EXISTS inference_images\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "295780cc-d644-4c11-b450-3fe68562b290",
   "metadata": {
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": "db = session.get_current_database()\nsc = session.get_current_schema()\nprint (f'database: {db}, schema: {sc}')\n\ndefault_model = mr.get_model(\"ChurnDetector\").default\nimg_repo = f'{db}.{sc}.inference_images'\n\nreg_prod_model = mr.log_model(model= default_model,\n                model_name= \"ChurnDetector\",\n                version_name= \"SPCS_INF\",\n                conda_dependencies=[\"snowflake-ml-python\", \"scikit-learn\",\"xgboost\"],\n                sample_input_data = training_dataset_sdf.select(columns).limit(100),\n                #options={\"relax_version\": False, \"enable_explainability\": True},\n                task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n                comment=\"Model to detect what customers will not buy again\"\n                )\n\n\nreg_prod_model.create_service(service_name=\"ChurnDetector_Prod\",\n                  service_compute_pool=\"CPU_XS_INFERENCE2\",\n                  image_repo=img_repo,\n                  ingress_enabled=True,\n                  gpu_requests=None)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8be6de17-2244-46b8-bdcd-5d86410aabf4",
   "metadata": {
    "language": "sql",
    "name": "cell38"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7c105c7-5e8a-4e38-b1c7-0eb8f3d409e9",
   "metadata": {
    "language": "sql",
    "name": "cell40",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}