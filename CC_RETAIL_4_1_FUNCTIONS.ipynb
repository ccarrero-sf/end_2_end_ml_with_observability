{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "no53j3f2jya6yzhuogkw",
   "authorId": "5744486210470",
   "authorName": "CCARRERO",
   "authorEmail": "carlos.carrero@snowflake.com",
   "sessionId": "08243150-c4d6-43fa-9ab9-d48c1dd987b3",
   "lastEditTime": 1744072756378
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "source": "CREATE OR REPLACE SCHEMA UTILS;\nUSE SCHEMA UTILS;\n\nCREATE OR REPLACE STAGE ML_STAGE\n  DIRECTORY = (ENABLE = TRUE)\n  ENCRYPTION = ( TYPE = 'SNOWFLAKE_SSE' );",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "codeCollapsed": false
   },
   "source": "import pandas as pd\nimport random\nfrom datetime import datetime, timedelta\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark import functions as F\nfrom snowflake import snowpark\n\ndef uc01_feature_engineering(session: snowpark.Session, db: str, sc:str, cur_date: datetime, table_name: str):\n\n    # Function to create features that define a profile of customer behavior based on latest purchases\n    # Features created for a given date (cur_date)\n    # New features added into table (table_name)\n\n    table_name = f'{db}.{sc}.{table_name}'\n    \n    # Load data\n    customers_tbl = '.'.join([db, sc,'CUSTOMERS'])\n    sales_tbl = '.'.join([db, sc,'SALES'])\n    feedback_tbl = '.'.join([db, sc,'FEEDBACK_SENTIMENT'])\n    \n    customers_df = session.table(customers_tbl)\n    sales_df = session.table(sales_tbl)\n    sales_df_last_tran = session.table(sales_tbl)\n\n    feedback_sentiment_df = session.table(feedback_tbl)    \n\n    # we are only doing feature engineering for transactions before cur_date\n    \n    sales_df_last_tran = sales_df_last_tran.filter(F.col(\"transaction_date\") < F.lit(cur_date))\n\n    sales_df = sales_df.filter(F.col(\"transaction_date\") < F.lit(cur_date ))\n        \n    # count only feedback before cur_date\n    \n    feedback_sentiment_df = feedback_sentiment_df.filter(F.col(\"chat_date\") < F.lit(cur_date))\n    \n    sales_agg_df = (\n        sales_df_last_tran.group_by(\"customer_id\")\n        .agg(\n            F.max(\"transaction_date\").alias(\"last_purchase_date\"),\n            F.sum(\"total_amount\").alias(\"total_customer_value\")\n        )\n    )\n    \n    def custom_column_naming(input_col, agg, window):\n        return f\"{agg}_{input_col}_{window.replace('-', 'past_')}\"\n                                                   \n    sales_agg_orders_df = sales_df.analytics.time_series_agg(\n            time_col=\"transaction_date\",\n            aggs={\"total_amount\": [\"SUM\", \"COUNT\"]},\n            windows=[\"-7D\",\"-1MM\", \"-2MM\", \"-3MM\"],\n            sliding_interval=\"1D\",\n            group_by=[\"CUSTOMER_ID\"],\n            col_formatter = custom_column_naming)\n\n    sales_agg_last_purchase = sales_agg_df.join(\n        sales_agg_orders_df,\n        (sales_agg_df.last_purchase_date == sales_agg_orders_df.transaction_date) &\n        (sales_agg_df.CUSTOMER_ID == sales_agg_orders_df.CUSTOMER_ID),\n        \"left\").select(\n            sales_agg_df[\"customer_id\"].alias(\"CUSTOMER_ID\"),\n            sales_agg_df[\"total_customer_value\"],\n            sales_agg_df[\"last_purchase_date\"],\n            sales_agg_orders_df[\"SUM_TOTAL_AMOUNT_PAST_7D\"],\n            sales_agg_orders_df[\"SUM_TOTAL_AMOUNT_PAST_1MM\"],\n            sales_agg_orders_df[\"SUM_TOTAL_AMOUNT_PAST_2MM\"],\n            sales_agg_orders_df[\"SUM_TOTAL_AMOUNT_PAST_3MM\"],\n            sales_agg_orders_df[\"COUNT_TOTAL_AMOUNT_PAST_7D\"],\n            sales_agg_orders_df[\"COUNT_TOTAL_AMOUNT_PAST_1MM\"],\n            sales_agg_orders_df[\"COUNT_TOTAL_AMOUNT_PAST_2MM\"],\n            sales_agg_orders_df[\"COUNT_TOTAL_AMOUNT_PAST_3MM\"]\n        )\n\n    #  feedback data\n\n    latest_feedback_df = (feedback_sentiment_df.group_by(\"customer_id\")\n            .agg(F.max(\"chat_date\").alias(\"chat_date\")))\n    \n    feedback_agg_df = feedback_sentiment_df.analytics.moving_agg(\n            aggs={\"SENTIMENT\": [\"MIN\", \"AVG\"]},\n            window_sizes=[2, 3, 4],\n            order_by=[\"chat_date\"],\n            group_by=[\"CUSTOMER_ID\"])\n\n    \n    feedback_agg_latest_df = latest_feedback_df.join(\n        feedback_agg_df, \"customer_id\", \"left\").select(\n            latest_feedback_df[\"CUSTOMER_ID\"].alias(\"CUSTOMER_ID\"),\n            feedback_agg_df[\"SENTIMENT_MIN_2\"],\n            feedback_agg_df[\"SENTIMENT_MIN_3\"],\n            feedback_agg_df[\"SENTIMENT_MIN_4\"],\n            feedback_agg_df[\"SENTIMENT_AVG_2\"],\n            feedback_agg_df[\"SENTIMENT_AVG_3\"],\n            feedback_agg_df[\"SENTIMENT_AVG_4\"],         \n        )\n    \n    # Join tables\n    features_df = (\n        customers_df.join(sales_agg_last_purchase, \"customer_id\", \"left\")\n        .join(feedback_agg_latest_df, \"customer_id\", \"left\")\n        .select(\n            customers_df[\"customer_id\"],\n            customers_df[\"age\"],\n            customers_df[\"gender\"],\n            customers_df[\"location\"],\n            customers_df[\"customer_segment\"],\n            sales_agg_last_purchase[\"last_purchase_date\"],\n            feedback_agg_latest_df[\"SENTIMENT_MIN_2\"],\n            feedback_agg_latest_df[\"SENTIMENT_MIN_3\"],\n            feedback_agg_latest_df[\"SENTIMENT_MIN_4\"],\n            feedback_agg_latest_df[\"SENTIMENT_AVG_2\"],\n            feedback_agg_latest_df[\"SENTIMENT_AVG_3\"],\n            feedback_agg_latest_df[\"SENTIMENT_AVG_4\"],\n            sales_agg_last_purchase[\"SUM_TOTAL_AMOUNT_PAST_7D\"],\n            sales_agg_last_purchase[\"SUM_TOTAL_AMOUNT_PAST_1MM\"],\n            sales_agg_last_purchase[\"SUM_TOTAL_AMOUNT_PAST_2MM\"],\n            sales_agg_last_purchase[\"SUM_TOTAL_AMOUNT_PAST_3MM\"],\n            sales_agg_last_purchase[\"COUNT_TOTAL_AMOUNT_PAST_7D\"].alias(\"COUNT_ORDERS_PAST_7D\"),\n            sales_agg_last_purchase[\"COUNT_TOTAL_AMOUNT_PAST_1MM\"].alias(\"COUNT_ORDERS_PAST_1MM\"),\n            sales_agg_last_purchase[\"COUNT_TOTAL_AMOUNT_PAST_2MM\"].alias(\"COUNT_ORDERS_PAST_2MM\"),\n            sales_agg_last_purchase[\"COUNT_TOTAL_AMOUNT_PAST_3MM\"].alias(\"COUNT_ORDERS_PAST_3MM\"),\n            F.datediff(\"day\", sales_agg_df[\"last_purchase_date\"], F.lit(cur_date)).alias(\"DAYS_SINCE_LAST_PURCHASE\"),\n            F.lit(cur_date).alias(\"TIMESTAMP\")\n        ).filter(sales_agg_df[\"last_purchase_date\"].isNotNull()  # Avoid customers never purchased\n        ).dropDuplicates([\"customer_id\", \"TIMESTAMP\"])  # Ensure one combination of customer_id and TIMESTAMP\n\n    )\n    \n    # Fill with 0 those where we have no data (so neutral feedback and zero iterations and amount)\n    columns_to_fill = [\n        \"SENTIMENT_MIN_2\", \"SENTIMENT_MIN_3\", \"SENTIMENT_MIN_4\", \"SENTIMENT_AVG_2\",\n        \"SENTIMENT_AVG_3\", \"SENTIMENT_AVG_4\",\n        \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n        \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n    ]\n    \n    for column in columns_to_fill:\n        features_df = features_df.fillna({column: 0})\n    \n    # Write to Snowflake Table\n    features_df.write.mode(\"append\").save_as_table(table_name)\n\n    print (f'Created table {table_name}')\n\nsession.sproc.register(\n    func=uc01_feature_engineering,\n    name=\"uc01_feature_engineering_sproc\",\n    replace=True,\n    is_permanent=True,\n    stage_location=\"@ML_STAGE\",\n    packages=['snowflake-snowpark-python', 'snowflake-ml-python'],\n    return_type=T.StringType()\n)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7df91c14-0dd9-40ab-9ca6-866d4f9aabb9",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "def uc01_label_churn(session: snowpark.Session, db: str, sc:str, baseline_table: str, output_table: str,  \n                     num_days_churn: int):\n\n    # Load baseline features dataset\n    baseline_table = f'{db}.{sc}.{baseline_table}'\n    output_table = f'{db}.{sc}.{output_table}'\n    \n    baseline_df = session.table(baseline_table)\n\n    # Load sales dataset\n    sales_df = session.table(f'{db}.{sc}.SALES')\n\n    # Filter sales to retain only customer ID and transaction date\n    sales_filtered = sales_df.select(F.col(\"CUSTOMER_ID\"), F.col(\"TRANSACTION_DATE\"))\n\n    # Find the next transaction date for each (CUSTOMER_ID, TIMESTAMP)\n    next_transaction_df = (\n        baseline_df\n        .join(sales_filtered, \"CUSTOMER_ID\", \"left\")\n        .filter(F.col(\"TRANSACTION_DATE\") >F.col(\"LAST_PURCHASE_DATE\"))\n        .group_by(F.col(\"CUSTOMER_ID\"), F.col(\"TIMESTAMP\"))\n        .agg(F.min(\"TRANSACTION_DATE\").alias(\"NEXT_TRANSACTION_DATE\"))\n    )\n\n    # Join back with the baseline dataset to compute CHURNED\n    final_df = (\n        baseline_df\n        .join(next_transaction_df, [\"CUSTOMER_ID\", \"TIMESTAMP\"], \"left\")\n        .select(\n            baseline_df[\"*\"],\n            F.when(\n                (F.col(\"NEXT_TRANSACTION_DATE\").is_null()) |\n                ((F.col(\"NEXT_TRANSACTION_DATE\") - F.col(\"LAST_PURCHASE_DATE\")) > num_days_churn),\n                1\n            ).otherwise(0).alias(\"CHURNED\"),\n            F.col(\"NEXT_TRANSACTION_DATE\")\n        )\n    )\n\n    # Save the final labeled dataset\n    final_df.write.mode(\"overwrite\").save_as_table(output_table)\n\nsession.sproc.register(\n    func=uc01_label_churn,\n    name=\"uc_01_label_churn_sproc\",\n    replace=True,\n    is_permanent=True,\n    stage_location=\"@ML_STAGE\",\n    packages=['snowflake-snowpark-python', 'snowflake-ml-python'],\n    return_type=T.StringType()\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bac79951-849e-4a48-a3ae-6338bf4142f0",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "def uc01_update_label_churn(session: snowpark.Session, db: str, sc:str, baseline_table: str,  \n                     num_days_churn: int):\n\n    # Load baseline features dataset\n    baseline_table = f'{db}.{sc}.{baseline_table}'\n    \n    baseline_df = session.table(baseline_table)\n\n    # Load sales dataset\n    sales_df = session.table(f'{db}.{sc}.SALES')\n\n    # Filter sales to retain only customer ID and transaction date\n    sales_filtered = sales_df.select(F.col(\"CUSTOMER_ID\"), F.col(\"TRANSACTION_DATE\"))\n\n    # Find the next transaction date for each (CUSTOMER_ID, TIMESTAMP)\n    next_transaction_df = (\n        baseline_df\n        .join(sales_filtered, \"CUSTOMER_ID\", \"left\")\n        .filter(F.col(\"TRANSACTION_DATE\") >F.col(\"LAST_PURCHASE_DATE\"))\n        .group_by(F.col(\"CUSTOMER_ID\"), F.col(\"TIMESTAMP\"))\n        .agg(F.min(\"TRANSACTION_DATE\").alias(\"NEXT_TX_DATE\"))\n    )\n\n    # Join back with the baseline dataset to compute CHURNED\n    final_df = (\n        baseline_df\n        .join(next_transaction_df, [\"CUSTOMER_ID\", \"TIMESTAMP\"], \"left\")\n        .select(\n            baseline_df[\"CUSTOMER_ID\"],\n            baseline_df[\"TIMESTAMP\"],\n            next_transaction_df[\"NEXT_TX_DATE\"],\n            F.when(\n                next_transaction_df[\"NEXT_TX_DATE\"].is_null() |\n                ((next_transaction_df[\"NEXT_TX_DATE\"] - baseline_df[\"LAST_PURCHASE_DATE\"]) > num_days_churn),\n                1\n            ).otherwise(0).alias(\"CHURNED\")\n        )    \n        .with_column_renamed(\"NEXT_TX_DATE\", \"NEXT_TRANSACTION_DATE\")\n\n    )\n\n    final_df.write.mode(\"overwrite\").save_as_table('temp_updates')\n\n    update_statement = f\"\"\"\n        update {baseline_table} c\n        set CHURNED = t.CHURNED,\n            NEXT_TRANSACTION_DATE = t.NEXT_TRANSACTION_DATE\n        from temp_updates t\n        where c.CUSTOMER_ID = t.CUSTOMER_ID AND\n            c.TIMESTAMP = t.TIMESTAMP\n          \n        \"\"\"\n\n    session.sql(update_statement).collect()\n\nsession.sproc.register(\n    func=uc01_update_label_churn,\n    name=\"uc01_update_label_churn_sproc\",\n    replace=True,\n    is_permanent=True,\n    stage_location=\"@ML_STAGE\",\n    packages=['snowflake-snowpark-python', 'snowflake-ml-python'],\n    return_type=T.StringType()\n)",
   "execution_count": null
  }
 ]
}